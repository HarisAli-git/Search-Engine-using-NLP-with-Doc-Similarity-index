{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def processString(t_pool, d_pool, it, x, flag, i, l):\n",
    "    position_no=[]\n",
    "    doc_no=[]\n",
    "    dlen = len(d_pool)\n",
    "    for j in range(dlen):\n",
    "        dilen = len(d_pool[j])\n",
    "        for k in range(dilen):\n",
    "            if t_pool[i][l] in d_pool[j][k]:\n",
    "                doc_no.append(j + it)\n",
    "                position_no.append(k + it)\n",
    "    setdn = set(doc_no)\n",
    "    l1 = len(doc_no)\n",
    "    l2 = len(list(setdn))\n",
    "    string = (str(x) + ' ' + str(l2) + ' '+ str(l2))\n",
    "    x += 1\n",
    "    y = 0\n",
    "    for j1 in range(l1):\n",
    "        dj1 = doc_no[j1]\n",
    "        pj1 = position_no[j1]\n",
    "        diff2 = position_no[j1] - position_no[j1-1]\n",
    "        diff1 = doc_no[j1] - doc_no[j1-1]\n",
    "        if flag == False:\n",
    "            string += ' ' + str(dj1 + 1) + ',' + str(pj1 + 1)\n",
    "        elif flag == True:\n",
    "            y += 1\n",
    "            if y == 1:        \n",
    "                string += ' ' + str(dj1) + ',' + str(pj1)\n",
    "            elif diff1 == 0:\n",
    "                string += ' ' + str(diff1) + ',' + str(diff2)\n",
    "            else:\n",
    "                string += ' ' + str(diff1) + ',' + str(pj1)\n",
    "    return str(string + '\\n'), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_processing(t_pool, d_pool, fl):\n",
    "    x1 = 0\n",
    "    x = 0\n",
    "    flag = False\n",
    "    file = open(r\"C:\\Users\\pc\\New Folder\\assignment1\\doc_index\",\"w\")\n",
    "    if fl == True:\n",
    "        file = open(r\"C:\\Users\\pc\\New Folder\\assignment1\\Hash_doc_index\",\"w\")\n",
    "        x1 = 1\n",
    "        flag = True\n",
    "    len11 = len(t_pool)\n",
    "    for i1 in range(len11):\n",
    "        len12 = len(t_pool[i1])\n",
    "        for jj in range(len12):\n",
    "            s1, x = processString(t_pool, d_pool, x1, x, flag, i1, jj)\n",
    "            file.write(s1)\n",
    "    file.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDetails(index, word, path):\n",
    "    print(\"\\nThe word to find: \", word)\n",
    "    print(\"\\nTerm Index : \", str(index), \"\\n\")\n",
    "    file = open(path +'\\doc_index', \"r\")\n",
    "    for x in ( raw.strip().split() for raw in file ):\n",
    "        if x[0] == index:\n",
    "            print(\"Number of documents containing the word: \", str(x[2]), \"\\n\")\n",
    "            print(\"Number of times word repeated in the dataset: \", str(x[1]), \"\\n\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchNLP(pool, text):\n",
    "    path = r\"C:\\Users\\pc\\New Folder\\assignment1\"\n",
    "    my_dic={}\n",
    "    i = 0\n",
    "    print(\"Assigning Term IDs in my_dic....................\")\n",
    "    for x in pool:\n",
    "        for y in x: \n",
    "            my_dic.update({str(y):str(i)})\n",
    "            i = i + 1\n",
    "    doc_processing(pool, text, True)\n",
    "    doc_processing(pool, text, False)\n",
    "    \n",
    "    flag = True\n",
    "    while flag == True:\n",
    "        print(\"\\n\\n\\nEnter the word to search or enter -1 to halt: \")\n",
    "        in_word = input()\n",
    "        if in_word == \"-1\":\n",
    "            print(\"\\nSearch Halted!\\n\\nHave a nice day!\")\n",
    "            return\n",
    "        dic_in = my_dic.get(in_word)\n",
    "        if printDetails(dic_in, in_word, path) == False:\n",
    "            print('Sorry, The word Entered word is not present\\nPlease try a different word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/pc//New Folder/assignment1/dataset/\n",
      "The files are as follow:\n",
      "\n",
      "['a.txt', 'b.txt', 'c.txt']\n",
      "\n",
      "\n",
      "Obtaining files from Data Set...\n",
      "\n",
      "pers, â€˜--Mystery, ancient and modern, with\n",
      "Seaography: then Drawling--the Drawling-master was an o\n",
      "\n",
      "\n",
      "Tokenization...\n",
      "\n",
      "\n",
      "Eliminating Stopwords\n",
      "\n",
      "\n",
      "Stemming...\n",
      "\n",
      "\n",
      "Removing Duplicates...\n",
      "\n",
      "\n",
      "Assigning Term IDs...\n",
      "\n",
      "\n",
      "Assigning Document IDs...\n",
      "\n",
      "Assigning Term IDs in a dictionary\n",
      "Max value in Doc :  1.436\n",
      "Max value in Doc :  1.436\n",
      "Max value in Doc :  1.61\n",
      "\n",
      "\n",
      "Normalised Document Vectors Are:\n",
      "\n",
      "[array([[0.696, 0.813, 0.696, ..., 0.   , 0.   , 0.696]]), array([[0.696, 0.813, 0.696, ..., 0.   , 0.   , 0.696]]), array([[0.621, 0.   , 0.621, ..., 0.   , 0.621, 0.621]])]\n",
      "\n",
      "\n",
      "Your Doc-Doc incidence matrix is:\n",
      "\n",
      "1.0\t0.99\t0.504\t\n",
      "0.99\t1.0\t0.506\t\n",
      "0.504\t0.506\t1.0\t\n",
      "Assigning Term IDs in my_dic....................\n",
      "\n",
      "\n",
      "\n",
      "Enter the word to search or enter -1 to halt: \n",
      "lark\n",
      "\n",
      "The word to find:  lark\n",
      "\n",
      "Term Index :  None \n",
      "\n",
      "Sorry, The word Entered word is not present\n",
      "Please try a different word\n",
      "\n",
      "\n",
      "\n",
      "Enter the word to search or enter -1 to halt: \n",
      "stop\n",
      "\n",
      "The word to find:  stop\n",
      "\n",
      "Term Index :  1739 \n",
      "\n",
      "Number of documents containing the word:  2 \n",
      "\n",
      "Number of times word repeated in the dataset:  2 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Enter the word to search or enter -1 to halt: \n",
      "work\n",
      "\n",
      "The word to find:  work\n",
      "\n",
      "Term Index :  3208 \n",
      "\n",
      "Number of documents containing the word:  3 \n",
      "\n",
      "Number of times word repeated in the dataset:  3 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Enter the word to search or enter -1 to halt: \n",
      "-1\n",
      "\n",
      "Search Halted!\n",
      "\n",
      "Have a nice day!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from stemming.porter2 import stem\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#preprocessing data\n",
    "#----------------------------------------------------------------------\n",
    "def getFilesInDir(dir): \n",
    "\tlst = []   \n",
    "\tfor file in os.listdir(dir):\n",
    "\t\tlst.append(file)\n",
    "\treturn lst\n",
    "\n",
    "def getParsedDocs(dir):\n",
    "\tprint(dir)\n",
    "\tfiles = getFilesInDir(dir)\n",
    "\tprint(\"The files are as follow:\\n\")\n",
    "\tprint(files)\n",
    "\tprint(\"\\n\")    \n",
    "\tdocs = []\n",
    "\n",
    "\tfor file in files:      \n",
    "\t\ttext = open(dir+\"//\"+file, 'r', encoding=\"utf8\").read()\n",
    "\t\tdocs.append(text)\n",
    "\treturn docs\n",
    "\n",
    "#tokenization--------------------------------------------------------------\n",
    "def tokenize(docs):\n",
    "\n",
    "\tdocs_tokens=[]\n",
    "\tfor doc in docs:\n",
    "\t\ts = re.findall(r'\\w+',doc)\n",
    "\t\tdocs_tokens.append(s)\n",
    "\n",
    "\ttemp_tokens = []\n",
    "\tpool = []\n",
    "\tfor i in range(len(docs_tokens)):\n",
    "\t\tfor j in range(len(docs_tokens[i])):\n",
    "\t\t\tdocs_tokens[i][j] = docs_tokens[i][j].lower()\n",
    "\treturn docs_tokens\n",
    "\n",
    "#-------Remove stopwords\n",
    "def removeStopWords(pool):\n",
    "\n",
    "\tfile = open (\"C:/Users/pc/New Folder/assignment1/stopwords.txt\",\"r\")\n",
    "\tcontent = file.readlines()\n",
    "\n",
    "\ttemp_content = []\n",
    "\n",
    "\tfor word in content:\n",
    "\t\tword = word[:-1]\n",
    "\t\ttemp_content.append(word)\n",
    "\tstop_list = temp_content\n",
    "    \n",
    "\n",
    "\ttemp_pool = []\n",
    "\tfor doc_tokens in pool:\n",
    "\t\t for word in stop_list:\n",
    "\t\t \tif word in doc_tokens:\n",
    "\t\t \t\tremoveAll(doc_tokens, word)\n",
    "\t\t temp_pool.append(doc_tokens)\n",
    "        \n",
    "\tpool = temp_pool \n",
    "\treturn pool\n",
    "\n",
    "def removeEmptySpace(pool):\n",
    "\treturn pool\n",
    "\ttemp_pool = []\n",
    "\tprint(\"Please Wait, this part takes the longest time\")\n",
    "\tfor doc_tokens in pool:\n",
    "\t\tc = ''\n",
    "\t\twhile c in doc_tokens:\n",
    "\t\t\tindex = doc_tokens.index(c)\n",
    "\t\t\tdoc_tokens.pop(index)\n",
    "\t\ttemp_pool.append(doc_tokens)\n",
    "\treturn temp_pool\n",
    "\n",
    "def removeAll(lst, token):\n",
    "\twhile token in lst:\n",
    "\t\tlst.remove(token)\n",
    "\treturn lst\n",
    "\n",
    "#--------------Stemming function to reduce inflected words\n",
    "def stemming(pool):\n",
    "\ttemp_pool = []\n",
    "\tfor doc_tokens in pool:\n",
    "\t\ttemp_doc = []\n",
    "\t\tfor token in doc_tokens:\n",
    "\t\t\ttoken = stem(token)\n",
    "\t\t\ttemp_doc.append(token)\n",
    "\t\ttemp_pool.append(temp_doc)\n",
    "\treturn temp_pool\t\n",
    "\n",
    "#function to remove duplicate words\n",
    "def removeDuplicates(pool):\n",
    "\ttemp_pool = []\n",
    "\tfor doc in pool:\n",
    "\t\ttemp_pool.append(list(set(doc)))\n",
    "\treturn temp_pool\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#Assigning termIDs to dictionary(Tokenized words)\n",
    "def assignTermid(pool):\n",
    "\tfile = open(r\"TermID.txt\",\"w+\")\n",
    "\ti = 1\n",
    "\tfor docs in pool:\n",
    "\t\tfor token in docs: \n",
    "\t\t\tfile.write(str(i) + '\\t' + token + \"\\n\")\n",
    "\t\t\ti += 1\n",
    "\tfile.close()\n",
    "#--------------------------------------------------------------\n",
    "#Assigning docID\n",
    "def assignDocids(dir):\n",
    "\tfile_dict = assignId(dir)\n",
    "\tfile_names = getFilesInDir(dir)\n",
    "\tfile = open(r\"DocID.txt\",\"w+\")\n",
    "\n",
    "\ti = 0\n",
    "\tfor name in file_names:\n",
    "\t\tfile.write(name +\"\\t\"+ str(file_dict[i][name]) + \"\\n\")\n",
    "\t\ti += 1\n",
    "\tfile.close()\n",
    "\n",
    "def assignId(dir):\n",
    "\tfile_names = getFilesInDir(dir)\n",
    "\tfile_dict = []\n",
    "\ti = 0\n",
    "\tfor file in file_names:\n",
    "\t\ti += 1\n",
    "\t\tfile_dict.append({file : i})\n",
    "\treturn file_dict\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#      INVERTED INDEX AND DOCUMENT FREQUENCY RECORDING\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "def usingdict(pool):\n",
    "\tdic={}\n",
    "\ti = 1\n",
    "\tprint(\"Assigning Term IDs in a dictionary\")\n",
    "\tfor docs in pool:\n",
    "\t\tfor token in docs: \n",
    "\t\t\tdic.update({str(token):str(i)})\n",
    "\t\t\ti += 1\n",
    "\treturn dic\n",
    "\n",
    "\n",
    "def computeWeight(termF,totalD,freqInDoc):\n",
    "\tif termF>0:    \n",
    "\t\tidff= math.log10(totalD/freqInDoc)\n",
    "\t\tweight= 1 + (math.log10(termF)) * idff\n",
    "\t\tidff=round(idff,3)\n",
    "\t\tweight=round(weight,3)\n",
    "\t\treturn weight       \n",
    "\telse:\n",
    "\t\treturn 0        \n",
    "\n",
    "#\treturn weight\n",
    "\n",
    "def normalizedWeight(array,maxWeight): \n",
    "    normalisedWeight=  (array/maxWeight)\n",
    "    return np.round(normalisedWeight,3)\n",
    "\n",
    "\n",
    "def cosineSimilarity(array):\n",
    "\tfor i in array:\n",
    "\t\tfor j in array:\n",
    "\t\t\tprint(round(cosine_similarity(i,j)[0][0] ,3),end='\\t')\n",
    "\t\tprint('')        \n",
    "\n",
    "def processPositions(token_pool, doc_pool):\n",
    "\tfile = open(r\"InvertedIndex.txt\",\"w+\")\n",
    "\toutputFormat=\"<TermID> <Token> <Frequency> <OccuranceInUniqueDocs> <DocNum : CountInThatDoc>\\n\"    \n",
    "\tfile.write(outputFormat)\n",
    "\tm=0\n",
    "\tdocLength=(len(token_pool))\n",
    "\tweightedWordsArray=[]  \n",
    "\tnormalisedWordsArray=[]\n",
    "\tfor i in range(len(token_pool)):\n",
    "\t\tfor l in range(len(token_pool[i])):\n",
    "\t\t\tposno=[]\n",
    "\t\t\tdocno=[]\n",
    "\t\t\tcountss=[]\n",
    "\t\t\tinDocument=[]\n",
    "\t\t\tfor j in range(len(doc_pool)):\n",
    "\t\t\t\tcountInDoc=0\n",
    "\t\t\t\tfor k in range(len(doc_pool[j])):\n",
    "\t\t\t\t\tif token_pool[i][l] in doc_pool[j][k]:\n",
    "\t\t\t\t\t\tdocno.append(j)\n",
    "\t\t\t\t\t\tposno.append(k)\n",
    "\t\t\t\t\t\tcountInDoc+=1\n",
    "\t\t\t\tcountss.append(countInDoc)\n",
    "\t\t\t\tinDocument.append(j)\n",
    "\t\t\tsetofno=list(set(docno))\n",
    "\t\t\tm=m+1   \n",
    "\t\t\tstring = str((m)) + '  ' + token_pool[i][l]+ \"  \" + str(len(docno)) + '  '+ str(len(setofno)) + \"\\t\"\n",
    "\t\t\tlendocno=len(docno)\n",
    "\t\t\tdocFreqForWordI=setofno\n",
    "\t\t\ttempWeight=[]\n",
    "\t\t\ttempNormalised=[]\n",
    "\t\t\tfor j in range(len(countss)):\n",
    "\t\t\t\ttermFrequencyforij =countss[j]\n",
    "\t\t\t\tweightedWord=computeWeight(termFrequencyforij, docLength ,len(docFreqForWordI))\n",
    "\t\t\t\ttempWeight.append(weightedWord)\n",
    "\t\t\t\tstring += '  ' + str(inDocument[j]+1) + ':' + str(countss[j]) + \",\"\n",
    "\t\t\tweightedWordsArray.append(tempWeight)\n",
    "\t\t\tstring+='\\n'\n",
    "\t\t\tfile.write(string)\n",
    "\tLISTSpecial =  np.array(weightedWordsArray).T  \n",
    "\tnormalisedWordsArray=[]\n",
    "\tfor m in LISTSpecial:\n",
    "\t\tprint(\"Max value in Doc : \",max(m))\n",
    "\t\tvec1 = np.array(normalizedWeight(m,max(m)))\n",
    "\t\tvec1 = np.reshape(vec1, (-1, len(vec1)))\n",
    "\t\tnormalisedWordsArray.append(vec1)\n",
    "\tprint(\"\\n\\nNormalised Document Vectors Are:\\n\")       \n",
    "\tprint(normalisedWordsArray)\n",
    "\tprint(\"\\n\\nYour Doc-Doc incidence matrix is:\\n\")  \n",
    "\tcosineSimilarity(normalisedWordsArray)           \n",
    "\tfile.close()            \n",
    "            \n",
    "#==================== MAIN PROGRAM\n",
    "def main():\n",
    "\t#dir2=argv[-1] \n",
    "\tdir = r\"C:/Users/pc//New Folder/assignment1/dataset/\"\n",
    "    \n",
    "\tdocs = getParsedDocs(dir)\n",
    "\tprint(\"Obtaining files from Data Set...\\n\")\n",
    "\tprint(docs[0][-100:])\n",
    "    \n",
    "\tpool = tokenize(docs)\n",
    "\ttext = pool\n",
    "\tprint(\"Tokenization...\\n\")    \n",
    "\tprint(\"\\nEliminating Stopwords\\n\")\n",
    "\tpool = removeStopWords(pool)    \n",
    "\tprint(\"\\nStemming...\\n\")    \n",
    "\tpool = stemming(pool) \n",
    "\tprint(\"\\nRemoving Duplicates...\\n\")\n",
    "\tpool = removeDuplicates(pool)\n",
    "\tprint(\"\\nAssigning Term IDs...\\n\")\n",
    "\tassignTermid(pool)   \n",
    "\tprint(\"\\nAssigning Document IDs...\\n\")  \n",
    "\tassignDocids(dir)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "\tdic=usingdict(pool)\n",
    "\tprocessPositions(pool, text)\n",
    "\tcomputeWeight(3,10,3)\n",
    "\tSearchNLP(pool, text)\n",
    "\treturn\n",
    "\n",
    "\n",
    "main()\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
