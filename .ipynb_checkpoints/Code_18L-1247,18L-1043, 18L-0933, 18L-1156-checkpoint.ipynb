{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def processString(t_pool, d_pool, it, x, flag, i, l):\n",
    "    position_no=[]\n",
    "    doc_no=[]\n",
    "    dlen = len(d_pool)\n",
    "    for j in range(dlen):\n",
    "        dilen = len(d_pool[j])\n",
    "        for k in range(dilen):\n",
    "            if t_pool[i][l] in d_pool[j][k]:\n",
    "                doc_no.append(j + it)\n",
    "                position_no.append(k + it)\n",
    "    setdn = set(doc_no)\n",
    "    l1 = len(doc_no)\n",
    "    l2 = len(list(setdn))\n",
    "    string = (str(x) + ' ' + str(l2) + ' '+ str(l2))\n",
    "    x += 1\n",
    "    y = 0\n",
    "    for j1 in range(l1):\n",
    "        dj1 = doc_no[j1]\n",
    "        pj1 = position_no[j1]\n",
    "        diff2 = position_no[j1] - position_no[j1-1]\n",
    "        diff1 = doc_no[j1] - doc_no[j1-1]\n",
    "        if flag == False:\n",
    "            string += ' ' + str(dj1 + 1) + ',' + str(pj1 + 1)\n",
    "        elif flag == True:\n",
    "            y += 1\n",
    "            if y == 1:        \n",
    "                string += ' ' + str(dj1) + ',' + str(pj1)\n",
    "            elif diff1 == 0:\n",
    "                string += ' ' + str(diff1) + ',' + str(diff2)\n",
    "            else:\n",
    "                string += ' ' + str(diff1) + ',' + str(pj1)\n",
    "    return str(string + '\\n'), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_processing(t_pool, d_pool, fl):\n",
    "    x1 = 0\n",
    "    x = 0\n",
    "    flag = False\n",
    "    file = open(r\"C:\\Users\\pc\\New Folder\\assignment1\\doc_index\",\"w\")\n",
    "    if fl == True:\n",
    "        file = open(r\"C:\\Users\\pc\\New Folder\\assignment1\\Hash_doc_index\",\"w\")\n",
    "        x1 = 1\n",
    "        flag = True\n",
    "    len11 = len(t_pool)\n",
    "    for i1 in range(len11):\n",
    "        len12 = len(t_pool[i1])\n",
    "        for jj in range(len12):\n",
    "            s1, x = processString(t_pool, d_pool, x1, x, flag, i1, jj)\n",
    "            file.write(s1)\n",
    "    file.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDetails(index, word, path):\n",
    "    print(\"\\nThe word to find: \", word)\n",
    "    print(\"\\nTerm Index : \", str(index), \"\\n\")\n",
    "    file = open(path +'\\doc_index', \"r\")\n",
    "    for x in ( raw.strip().split() for raw in file ):\n",
    "        if x[0] == index:\n",
    "            print(\"Number of documents containing the word: \", str(x[2]), \"\\n\")\n",
    "            print(\"Number of times word repeated in the dataset: \", str(x[1]), \"\\n\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchNLP(pool, text):\n",
    "    path = r\"C:\\Users\\pc\\New Folder\\assignment1\"\n",
    "    my_dic={}\n",
    "    i = 0\n",
    "    print(\"Assigning Term IDs in my_dic....................\")\n",
    "    for x in pool:\n",
    "        for y in x: \n",
    "            my_dic.update({str(y):str(i)})\n",
    "            i = i + 1\n",
    "    doc_processing(pool, text, True)\n",
    "    doc_processing(pool, text, False)\n",
    "    \n",
    "    flag = True\n",
    "    while flag == True:\n",
    "        print(\"\\n\\n\\nEnter the word to search or enter -1 to halt: \")\n",
    "        in_word = input()\n",
    "        if in_word == \"-1\":\n",
    "            print(\"\\nSearch Halted!\\n\\nHave a nice day!\")\n",
    "            return\n",
    "        dic_in = my_dic.get(in_word)\n",
    "        if printDetails(dic_in, in_word, path) == False:\n",
    "            print('Sorry, The word Entered word is not present\\nPlease try a different word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from stemming.porter2 import stem\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#preprocessing\n",
    "#----------------------------------------------------------------------\n",
    "def getFilesInDir(dir): \n",
    "\tlst = []   #this will contain names of all files in the data set\n",
    "\tfor file in os.listdir(dir):\n",
    "\t\tlst.append(file)\n",
    "\treturn lst\n",
    "\n",
    "#-------------------------------------------------------------- \n",
    "#this function returns list of documents containing all the text\n",
    "def getParsedDocs(dir):\n",
    "\tprint(dir)\n",
    "\tfiles = getFilesInDir(dir)\n",
    "\tprint(\"The files are as follow:\\n\")\n",
    "\tprint(files) #names of files obtain from data set\n",
    "\tprint(\"\\n\")    \n",
    "\tdocs = []\n",
    "\n",
    "\tfor file in files:      \n",
    "\t\ttext = open(dir+\"//\"+file, 'r', encoding=\"utf8\").read() #read data from all files, file by file\n",
    "\t\t#print(text[-100:])\n",
    "\t\tdocs.append(text) #every file will be appended in the list of documents eg: at docs[0]->file1 data | docs[1]->file2 data.. so on\n",
    "\treturn docs\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#tokenizer funtion-> tokenize the string of every document \n",
    "def tokenize(docs):\n",
    "\n",
    "\tdocs_tokens=[] #list of tokens, file by file\n",
    "\tfor doc in docs:\n",
    "\t\ts = re.findall(r'\\w+',doc) #gettig all tokkens(characters)\n",
    "\t\tdocs_tokens.append(s) #docs_tokens[0]->all tokens of doc1 | docs[1]-> all tokens of doc2.. so on\n",
    "\n",
    "\ttemp_tokens = []\n",
    "\tpool = []\n",
    "\tfor i in range(len(docs_tokens)):\n",
    "\t\tfor j in range(len(docs_tokens[i])):\n",
    "\t\t\tdocs_tokens[i][j] = docs_tokens[i][j].lower() #converting to lower case for ease\n",
    "            \n",
    "\treturn docs_tokens\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "#function to remove stopwords\n",
    "def removeStopList(pool):\n",
    "#\tpool = removeEmptySpace(pool)\n",
    "\n",
    "\tfile = open (\"C:/Users/pc/New Folder/assignment1/stopwords.txt\",\"r\")\n",
    "\tcontent = file.readlines()\n",
    "\n",
    "\ttemp_content = []\n",
    "\n",
    "\tfor word in content:\n",
    "\t\tword = word[:-1]\n",
    "\t\ttemp_content.append(word)\n",
    "\tstop_list = temp_content\n",
    "    \n",
    "\n",
    "\ttemp_pool = []\n",
    "\tfor doc_tokens in pool:\n",
    "\t\t for word in stop_list:\n",
    "\t\t \tif word in doc_tokens:\n",
    "\t\t \t\tremoveAll(doc_tokens, word)\n",
    "\t\t temp_pool.append(doc_tokens)\n",
    "        \n",
    "\tpool = temp_pool \n",
    "\treturn pool\n",
    "\n",
    "#_________ part of stop word removal\n",
    "def removeEmptySpace(pool):\n",
    "\treturn pool\n",
    "\ttemp_pool = []\n",
    "\tprint(\"Please Wait, this part takes the longest time\")\n",
    "\tfor doc_tokens in pool:\n",
    "\t\tc = ''\n",
    "\t\twhile c in doc_tokens:\n",
    "\t\t\tindex = doc_tokens.index(c)\n",
    "\t\t\tdoc_tokens.pop(index)\n",
    "\t\ttemp_pool.append(doc_tokens)\n",
    "\treturn temp_pool\n",
    "\n",
    "\n",
    "#_________ part of stop word removal\n",
    "def removeAll(lst, token):\n",
    "\twhile token in lst:\n",
    "\t\tlst.remove(token)\n",
    "\treturn lst\n",
    "\n",
    "#----------------------------------------------------------------Stemming\n",
    "#stemming function to reduce inflected words to their word stem\n",
    "def stemming(pool):\n",
    "\ttemp_pool = []\n",
    "\tfor doc_tokens in pool:\n",
    "\t\ttemp_doc = []\n",
    "\t\tfor token in doc_tokens:\n",
    "\t\t\ttoken = stem(token)\n",
    "\t\t\ttemp_doc.append(token)\n",
    "\t\ttemp_pool.append(temp_doc)\n",
    "\treturn temp_pool\t\n",
    "\n",
    "#function to remove duplicate words for better results\n",
    "def removeDuplicates(pool):\n",
    "\ttemp_pool = []\n",
    "\tfor doc in pool:\n",
    "\t\ttemp_pool.append(list(set(doc)))\n",
    "\treturn temp_pool\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#Assigning termIDs to dictionary(Tokenized words)\n",
    "def assignTermid(pool):\n",
    "\tfile = open(r\"TermID.txt\",\"w+\")\n",
    "\ti = 1\n",
    "\tfor docs in pool:\n",
    "\t\tfor token in docs: \n",
    "\t\t\tfile.write(str(i) + '\\t' + token + \"\\n\")\n",
    "\t\t\ti += 1\n",
    "\tfile.close()\n",
    "#--------------------------------------------------------------\n",
    "#Assigning docIDs to files of the data set\n",
    "def assignDocids(dir):\n",
    "\tfile_dict = assignId(dir)\n",
    "\tfile_names = getFilesInDir(dir)\n",
    "\tfile = open(r\"DocID.txt\",\"w+\")\n",
    "\n",
    "\ti = 0\n",
    "\tfor name in file_names:\n",
    "\t\tfile.write(name +\"\\t\"+ str(file_dict[i][name]) + \"\\n\")\n",
    "\t\ti += 1\n",
    "\tfile.close()\n",
    "\n",
    "def assignId(dir):\n",
    "\tfile_names = getFilesInDir(dir)\n",
    "\tfile_dict = []\n",
    "\ti = 0\n",
    "\tfor file in file_names:\n",
    "\t\ti += 1\n",
    "\t\tfile_dict.append({file : i})\n",
    "\treturn file_dict\n",
    "#--------------------------------------------------------------\n",
    "#      INVERTED INDEX AND DOCUMENT FREQUENCY RECORDING\n",
    "#--------------------------------------------------------------\n",
    "def usingdict(pool):\n",
    "\tdic={}\n",
    "\ti = 1\n",
    "\tprint(\"Assigning Term IDs in a dictionary\")\n",
    "\tfor docs in pool:\n",
    "\t\tfor token in docs: \n",
    "\t\t\tdic.update({str(token):str(i)})\n",
    "\t\t\ti += 1\n",
    "\treturn dic\n",
    "\n",
    "\n",
    "def computeWeight(termF,totalD,freqInDoc):\n",
    "\tif termF>0:    \n",
    "\t\tidff= math.log10(totalD/freqInDoc) #inverse document frequency\n",
    "\t\tweight= 1 + (math.log10(termF)) * idff\n",
    "\t\tidff=round(idff,3)\n",
    "\t\tweight=round(weight,3)\n",
    "\t\treturn weight       \n",
    "\telse: #since log(0) is undefined so if term count in doc is zero then weight is zero\n",
    "\t\treturn 0        \n",
    "\n",
    "#\treturn weight\n",
    "\n",
    "def normalizedWeight(array,maxWeight): \n",
    "    normalisedWeight=  (array/maxWeight)  #Broadcasting\n",
    "    return np.round(normalisedWeight,3)\n",
    "\n",
    "\n",
    "def cosineSimilarity(array):\n",
    "\tfor i in array:\n",
    "\t\tfor j in array:\n",
    "\t\t\tprint(round(cosine_similarity(i,j)[0][0] ,3),end='\\t')\n",
    "\t\tprint('')    \n",
    "#\tprint(\"\")     \n",
    "    \n",
    "\n",
    "\n",
    "def savePositions2(token_pool, doc_pool):\n",
    "\tfile = open(r\"InvertedIndex.txt\",\"w+\")\n",
    "\toutputFormat=\"<TermID> <Token> <Frequency> <OccuranceInUniqueDocs> <DocNum : CountInThatDoc>\\n\"    \n",
    "\tfile.write(outputFormat)\n",
    "\tm=0\n",
    "\tdocLength=(len(token_pool)) #for computing weight --> [N]\n",
    "#\tprint(docLength) \n",
    "\tweightedWordsArray=[]  \n",
    "\tnormalisedWordsArray=[]\n",
    "\tfor i in range(len(token_pool)):\n",
    "\t\tfor l in range(len(token_pool[i])):\n",
    "\t\t\tposno=[]\n",
    "\t\t\tdocno=[]\n",
    "\t\t\tcountss=[]\n",
    "\t\t\tinDocument=[]\n",
    "\t\t\tfor j in range(len(doc_pool)):\n",
    "\t\t\t\tcountInDoc=0\n",
    "\t\t\t\tfor k in range(len(doc_pool[j])):\n",
    "\t\t\t\t\tif token_pool[i][l] in doc_pool[j][k]:\n",
    "\t\t\t\t\t\tdocno.append(j)\n",
    "\t\t\t\t\t\tposno.append(k)\n",
    "\t\t\t\t\t\tcountInDoc+=1\n",
    "\t\t\t\tcountss.append(countInDoc)\n",
    "\t\t\t\tinDocument.append(j)\n",
    "\t\t\tsetofno=list(set(docno))\n",
    "\t\t\tm=m+1\n",
    "\t\t\t#          TermID/Locaton    ,  Token/term   , total freq in all docs  , occurance in unique docs= df(i)   \n",
    "\t\t\tstring = str((m)) + '  ' + token_pool[i][l]+ \"  \" + str(len(docno)) + '  '+ str(len(setofno)) + \"\\t\"\n",
    "\t\t\tlendocno=len(docno)\n",
    "\t\t\tdocFreqForWordI=setofno #for computing weight of word --> [df(i)]\n",
    "\t\t\ttempWeight=[]\n",
    "\t\t\ttempNormalised=[]\n",
    "\t\t\tfor j in range(len(countss)):\n",
    "\t\t\t\ttermFrequencyforij =countss[j]\n",
    "\t\t\t\t#print(\"CHECKKKK!\")\n",
    "\t\t\t\t#print(termFrequencyforij)\n",
    "\t\t\t\t#print(docLength)\n",
    "\t\t\t\t#print(token_pool[i][l]+ str(docno))\n",
    "\t\t\t\tweightedWord=computeWeight(termFrequencyforij, docLength ,len(docFreqForWordI))# tf(i,j),N,df(i)\n",
    "\t\t\t\ttempWeight.append(weightedWord)\n",
    "\t\t\t\t#                 document j,                 count of word i in document j = tf(i,j)\n",
    "\t\t\t\tstring += '  ' + str(inDocument[j]+1) + ':' + str(countss[j]) + \",\"  #+ \"  {\" + str(weightedWord) +\"}  \"\n",
    "\t\t\tweightedWordsArray.append(tempWeight)\n",
    "\t\t\t#print( \"Max value element : \", max(tempWeight)) #word-wise max weight in all docs\n",
    "\t\t\t#print(\"Max value element : \",max(weightedWordsArray))        \n",
    "\t\t\tstring+='\\n'\n",
    "\t\t\tfile.write(string)\n",
    "\tLISTSpecial =  np.array(weightedWordsArray).T  \n",
    "\tnormalisedWordsArray=[]\n",
    "\tfor m in LISTSpecial:\n",
    "\t\tprint(\"Max value in Doc : \",max(m)) #array of weighted words'\n",
    "\t\t#normalisedWordsArray.append(normalizedWeight(m,max(m)))\n",
    "\t\tvec1 = np.array(normalizedWeight(m,max(m)))\n",
    "\t\tvec1 = np.reshape(vec1, (-1, len(vec1)))\n",
    "\t\tnormalisedWordsArray.append(vec1)\n",
    "\tprint(\"\\n\\nNormalised Document Vectors Are:\\n\")       \n",
    "\tprint(normalisedWordsArray) # normalised doc vectors\n",
    "\t#cosineResults=[]\n",
    "\tprint(\"\\n\\nYour Doc-Doc incidence matrix is:\\n\")  \n",
    "\tcosineSimilarity(normalisedWordsArray)           \n",
    "\n",
    "    \n",
    "#\tprint(len(token_pool)\n",
    "#\tprint(inDocument)\n",
    "\tfile.close()            \n",
    "            \n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#====================\n",
    "#==================== MAIN PROGRAM\n",
    "def main():\n",
    "\t#dir2=argv[-1] \n",
    "\tdir = r\"C:/Users/pc//New Folder/assignment1/dataset/\"\n",
    "    \n",
    "\tdocs = getParsedDocs(dir)\n",
    "\tprint(\"Obtaining files from Data Set...\\n\")\n",
    "\tprint(docs[0][-100:]) #printing last ten characters of document 1.\n",
    "    \n",
    "\tpool = tokenize(docs)\n",
    "\ttext = pool\n",
    "\tprint(\"Tokenization...\\n\")    \n",
    "\tprint(\"\\nEliminating Stopwords\\n\")\n",
    "\tpool = removeStopList(pool)    \n",
    "\tprint(\"\\nStemming...\\n\")    \n",
    "\tpool = stemming(pool) \n",
    "\tprint(\"\\nRemoving Duplicates...\\n\")\n",
    "\tpool = removeDuplicates(pool)\n",
    "\tprint(\"\\nAssigning Term IDs...\\n\")\n",
    "\tassignTermid(pool)   \n",
    "\tprint(\"\\nAssigning Document IDs...\\n\")  \n",
    "\tassignDocids(dir)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "\tdic=usingdict(pool)\n",
    "\tsavePositions2(pool, text)\n",
    "\tcomputeWeight(3,10,3)\n",
    "\tSearchNLP(pool, text)\n",
    "\treturn\n",
    "\n",
    "\n",
    "main()\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
